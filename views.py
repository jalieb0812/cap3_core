from django.shortcuts import render
from django.http import HttpResponse
import nltk
#nltk.download('stopwords')
import spacy

from blackstone.pipeline.compound_cases import CompoundCases
from blackstone.pipeline.sentence_segmenter import SentenceSegmenter
from blackstone.rules import CITATION_PATTERNS

import os
import sys
sys.path.append('..')

import pandas as pd
import matplotlib.pyplot as plt
import lzma
import json
import urllib.request
import shutil
import math
import string
import time


from nltk.tokenize import word_tokenize
from .models import All_Cases
# Create your views here.
def get_top_cat(doc):
    """
    Function to identify the highest scoring category
    prediction generated by the text categoriser.
    """
    cats = doc.cats
    max_score = max(cats.values())
    max_cats = [k for k, v in cats.items() if v == max_score]
    max_cat = max_cats[0]
    return (max_cat, max_score)

def index(request):

    all_cases = All_Cases.objects.all()

    some_cases = all_cases[183000:183033]



    nlp = spacy.load("en_blackstone_proto")
    # add the Blackstone sentence_segmenter to the pipeline before the parser
    sentence_segmenter = SentenceSegmenter(nlp.vocab, CITATION_PATTERNS)
    nlp.add_pipe(sentence_segmenter, before="parser")

    compound_pipe = CompoundCases(nlp)
    nlp.add_pipe(compound_pipe)

    context = {

    'cases': all_cases,

    'some_cases': some_cases,

    'head_matters': [case.data['casebody']['data']['head_matter'] for case in some_cases]
    }



    """
    id
    urls
    name
    name_abbreviation
    'decision_date'
    'docker_number'
    'first_page'
    'last_page'
    'citations'
    'volume'
    'reporter'
    'court'
    'jurisdiction'
    'cites_to'
    'frontend_url'
    'preview'
    'casebody'{status; data{judges, attorneys, head_matter,
                corrections, opinions {
                text, author, type,
                }}
                parties}


    """




    return render(request, 'core/index.html', context)


def get_cats(cases):

    for case in cases:

        author = case.data['casebody']['data']['opinions'][0]['author']

        case_name = case.data['name']

        case_text = case.data['casebody']['data']['opinions'][0]['text'].split("\n")
        for text in case_text:
            doc = nlp(text)

            # Get the sentences in the passage of text
            sentences = [sent.text for sent in doc.sents]

            # Print the sentence and the corresponding predicted category.
            for sentence in sentences:
                doc = nlp(sentence)
                top_category = get_top_cat(doc)
                if top_category[0] != 'UNCAT':
                    print (f"\"{sentence}\" {top_category}\n")

def load_cases():


    with lzma.open("ill_text/data/data.jsonl.xz") as infile:

        i = 0

        while i < 183033:
            for line in infile:
                #decode the file into a convenient format
                print(f" on file number {i} \n")
                record = All_Cases(data = json.loads(str(line, 'utf-8')) )

                record.save()

                i += 1

                if i > 183033:
                    break

def get_citations():
    #sentences = dict()
    #
    # for case in all_cases[400:500]:
    #
    #     author = case.data['casebody']['data']['opinions'][0]['author']
    #
    #     case_name = case.data['name']
    #     for text in case.data['casebody']['data']['opinions'][0]['text'].split("\n"):
    #         for sentence in nltk.sent_tokenize(text):
    #         #for sentence in nlp(text):
    #             tokens = nlp(sentence)
    #             if tokens:
    #                 sentences[sentence] = tokens
    #             for compound_ref in tokens._.compound_cases:
    #                 print(compound_ref)
    #             for ent in sentences[sentence].ents:
    #                 if ent.label_ == "CASENAME":
    #                     casename = ent.text
    #                 else:
    #                     casename = "idk"
    #                 if ent.label_ == "CITATION":
    #                     print(f" author: {author} case: \
    #                     {case_name} labels: {ent.label_} {casename} citation: \
    #                     {ent.text} \n")

    return None
